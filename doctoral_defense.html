<!-- Title slide -->
<section>
<h3 style="color: #888">
    Doctoral Defense</h3>
<h1 style="margin-top: 0.5em; margin-bottom: 0em; color: #000">Tangible Landscape</h1>
<h5 style="margin-top: 0.25em;color: #000">a tangible interface for geospatial modeling</h5>
<h5 style="margin-top: 0.5em;color: #888">Brendan Harmon</h5>
<img height="80px" style="margin-top: 2em" src="img/logos/cgaBlack.png">
<h5 style="color: #888">North Carolina State University</h5>
</section>

<!-- About me -->


<!-- Introduction -->
<section>
<h2>Spatial thinking</h2> <!-- background: faded out image something spatial -->
<p>‘the mental processes of representing, analyzing, and drawing inferences from spatial relations’ [Uttal et al. 2013] </p>
<p><small>
  D.H. Uttal, D.I. Miller, and N.S. Newcombe. 2013. Exploring and Enhancing Spatial Thinking: Links to Achievement in Science, Technology, Engineering, and Mathematics? Curr. Dir. Psychol. Sci. 22, 5 (2013), 367–373. DOI:<a href="http://dx.doi.org/10.1177/0963721413484756">http://dx.doi.org/10.1177/0963721413484756</a>
</small></p>
</section>

<section>
<h2>3D spatial thinking</h2> <!-- background: faded out image of something 3D -->
<ul>
    <li>Used in <b>geology</b> to understand the structure of the earth</li>
    <li>Used in <b>ecology</b> to understand the structure of ecosystems</li>
    <li>Used in <b>civil engineering</b> to shape landscapes</li>
    <li>Used in <b>architecture</b> to design buildings</li>
    <li>Used in <b>urban planning</b> to model cities</li>
    <li>Used in <b>arts</b> to shape sculpture</li>
</ul>
</section>

<section>
<h2>Spatial computation</h2> <!-- background: 3D model in GIS (Centennial viewshed analysis) -->
<p>Spatial tasks can be performed computationally to efficiently store, model, and analyze large sets of spatial data and solve complex spatiotemporal problems.</p>
<ul>
    <li>Computer-aided design (CAD) and 3D modeling software are used to interactively, computationally model, analyze, and animate complex 3D forms</li>
    <li>Geographic information systems (GIS), geospatial programming, and spatial statistics are used to mathematically model, simulate, and optimize multidimensional spatial patterns and processes</li>
</ul>
</section>

<section> <!-- background: CLI - GUI - TUI -->
<h2>Human-computer interaction</h2>
<p>
CAD &amp; GIS can be unintuitive, challenging to use, and creatively constraining due to the complexity of the software, the complex workflows, and the limited modes of interaction and visualization <!--[Ratti et al. 2004a]-->
</p>
<p>
<ul>
    <li><b>Command Line Interface (CLI)</b></li>
    <li><b>Graphical User Interface (GUI)</b></li>
</ul>
</p>
<p><small>
Carlo Ratti et al. 2004. Tangible User Interfaces (TUIs): A Novel Paradigm for GIS. Trans. GIS 8, 4 (2004), 407–421. DOI:<a href="http://dx.doi.org/10.1111/j.1467-9671.2004.00193.x">http://dx.doi.org/10.1111/j.1467-9671.2004.00193.x</a>
</small></p>
</section>

<!--
Unintuitive interactions with GIS can frustrate users, constrain how they think about space, and add new cognitive burdens that require highly developed spatial skills and reasoning to overcome. The paradigmatic modes for interacting with GIS today – command line interfaces (CLI) and graphical user interfaces (GUI) – require physical input into devices like keyboards, mice, digitizing pens, and touch screens, but output data visually as text or graphics. Theoretically this disconnect between intention, action, and feedback makes graphical interaction unintuitive [Dourish 2001; Ishii 2008]. Since users can only think about space visually with GUIs, they need sophisticated spatial abilities like mental rotation [Shepard and Metzler 1971; Just and Carpenter 1985] to parse and understand, much less manipulate 3D space.
-->

<!-- style="opacity: 0.5; filter: alpha(opacity=50)" -->

<section data-background="img/figure/head_vr_grey_opac_50.png">
<h2>Natural interaction</h2>
<p style="color: #000"></p>
<ul>
    <li>Virtual Reality (VR)</li>
    <li>Augumented Reality (AR)</li>
    <li>Touch User Interfaces</li>
    <li>Tangible User Interfaces (TUI)</li>
</ul>
</p>
</section>

<section> <!-- data-background="" -->
<h2>Tangible interaction</h2>
<p>Couple physical and digital data</p>
<ul>
    <li>Enables embodied cognition</li>
    <li>Draws on existing motor schemas</li>
    <li>Seamlessly connects intention, action, and feedback</li>
    <li>Offload cognitive tasks like spatial perception and manipulation onto the body</li>
</ul>
<p><small>
David Kirsh. 2013. Embodied cognition and the magical future of interaction design. ACM Trans. Comput. Interact. 20, 1 (2013), 3:1–3:30. DOI:<a href="http://dx.doi.org/10.1145/2442106.2442109">http://dx.doi.org/10.1145/2442106.2442109</a>
</small></p>
</section>


<!--
In embodied cognition higher cognitive processes are grounded in, built upon, and mediated by bodily experiences such as kinaesthetic perception and action [Hardy-Vallee and Payette 2008]. Tangible interfaces – interfaces that couple physical and digital data [Dourish 2001] – are designed to enable embodied interaction by physically manifesting digital data so that users can cognitively grasp and absorb it, thinking with it rather than about it [Kirsh 2013]. Embodied interaction should be highly intuitive – drawing on existing motor schemas and seamlessly connecting intention, action, and feedback. It should reduce users’ cognitive load by enabling them to physically simulate processes and offload tasks like spatial perception and manipulation onto the body [Kirsh 2013]. Distance and physical properties like size, shape, volume, weight, hardness, and texture can be automatically and subconsciously assessed with the body [Jeannerod 1997]. Tangible interfaces should, therefore, enable users to subconsciously, kinaesethically judge and manipulate spatial distances, relationships, patterns, 3D forms, and volumes offloading these challenging cognitive tasks onto their bodies.
-->


<!-- Tangible interfaces for GIS -->
<section>
<h2>Tangible interfaces for geospatial modeling</h2>
<img class="stretch" src="img/system/tl_concept.png">
<p>Couple physical and digital geospatial models</p>
</section>

<!-- History -->

<!-- Actuated pin tables -->

<!-- XenoVision Mark III Dynamic Sand Table -->
<section>
<h2>Actuated pin tables</h2>
<img height="250px" src="img/history/xenovision_2.jpg">
<img height="250px" src="img/history/xenovision_3.gif">
<p><b>XenoVision Mark III Dynamic Sand Table, 2004</b></p>
<p><small>Source: <i class="fa fa-copyright" aria-hidden="true"></i> <a href="http://www.xenotran.com/">Xenotran</a></small></p>
</section>

<!-- inFORM -->
<section>
<h2>Actuated pin tables</h2>
<img class="stretch" src="img/history/inform.jpg">
<p><b>inFORM, 2013-present</b></p>
<p><small>Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii. 2013. inFORM: dynamic physical affordances and constraints through shape and object actuation. In Proceedings of the 26th annual ACM symposium on User interface software and technology - UIST ’13. New York, New York, USA: ACM Press, 417–426. DOI:<a href="http://dx.doi.org/10.1145/2501988.2502032">http://dx.doi.org/10.1145/2501988.2502032</a></small></p>
<p><small>Source: <i class="fa fa-creative-commons" aria-hidden="true"></i> <a href="http://tangible.media.mit.edu/project/inform/">Tangible Media Group, MIT Media Lab</a></small></p>
</section>

<!-- Augmented architectural models -->

<!-- Urp -->
<section>
<h2>Augmented architectural models</h2>
<img class="stretch" src="img/history/urp.png">
<p><b>Urp, 1996-2001</b></p>
<p><small>John Underkoffler and Hiroshi Ishii. 1999. Urp: a luminous-tangible workbench for urban planning and design. In CHI ’99 Proceedings of the SIGCHI conference on Human Factors in Computing Systems. New York, New York, USA: ACM Press, 386–393. DOI:<a href="http://dx.doi.org/10.1145/302979.303114">http://dx.doi.org/10.1145/302979.303114</a></small></p>
<p><small>Source: <i class="fa fa-copyright" aria-hidden="true"></i> <a href="https://www.media.mit.edu/projects/luminous-roomurp/">Tangible Media Group, MIT Media Lab</a></small></p>
</section>

<!-- Collaborative Design Platform -->
<section>
<h2>Augmented architectural models</h2>
<img class="stretch" src="img/history/cdp.jpg">
<p><b>Collaborative Design Platform, 2011-present</b></p>
<p><small>Gerhard Schubert, Sebastian Riedel, and Frank Petzold. 2013. Seamfully connected: Real working models as tangible interfaces for architectural design. In Global Design and Local Materialization. Springer-Verlag Berlin Heidelberg, 210–221. DOI:<a href="http://dx.doi.org/10.1007/978-3-642-38974-0_20">http://dx.doi.org/10.1007/978-3-642-38974-0_20</a></small></p>
<p><small>Source: <i class="fa fa-copyright" aria-hidden="true"></i> <a href="http://cdp.ai.ar.tum.de/">Dr.-Ing. Gerhard Schubert, Technische Universität München</a></small></p>
</section>

<!-- Augmented sandboxes -->

<!-- Sandscape and Illuminating Clay -->
<section>
<h2>Augmented sandboxes</h2>
<img height="250px" src="img/history/sandscape.png">
<img height="250px" src="img/history/illuminating_clay.png">
<p><b>Sandscape</b> &amp; <b>Illuminating Clay, 2002-2004</b></p>
<p><small>
H. Ishii, C. Ratti, B. Piper, Y. Wang, A. Biderman, and E. Ben-Joseph. 2004. Bringing Clay and Sand into Digital Design — Continuous Tangible user Interfaces. BT Technol. J. 22, 4 (2004), 287–299. DOI:<a href="http://dx.doi.org/10.1023/B:BTTJ.0000047607.16164.16">http://dx.doi.org/10.1023/B:BTTJ.0000047607.16164.16</a>
</small></p>
<p><small>
Source: <i class="fa fa-copyright" aria-hidden="true"></i> <a href="http://tangible.media.mit.edu/project/illuminating-clay/">Tangible Media Group, MIT Media Lab</a>
</small></p>
</section>

<!-- Tangible Geospatial Modeling System -->
<section>
<h2>Augmented sandboxes</h2>
<img height="250px" src="img/history/tangeoms_1.jpg">
<img height="250px" src="img/history/tangeoms_2.jpg">
<p><b>Tangible Geospatial Modeling System, 2006-2010</b></p>
<p><small>Laura Tateosian, Helena Mitasova, Brendan A. Harmon, Brent Fogleman, Katherine Weaver, and Russell S. Harmon. 2010. TanGeoMS: Tangible Geospatial Modeling System. IEEE Trans. Vis. Comput. Graph. 16, 6 (2010), 1605–12. DOI:<a href="http://dx.doi.org/10.1109/TVCG.2010.202">http://dx.doi.org/10.1109/TVCG.2010.202</a>
</small></p>
<p><small>Source: <i class="fa fa-creative-commons" aria-hidden="true"></i> <a href="http://www4.ncsu.edu/~hmitaso/wrriwork/tangis/tangis.html">NCSU GeoForAll Lab</a></small></p>
</section>

<!-- Augmented Reality Sandbox -->
<section>
<h2>Augmented sandboxes</h2>
<img class="stretch" src="img/history/SARndbox_1.jpg">
<p><b>Augmented Reality Sandbox, 2012-present</b></p>
<p><small>Source: <i class="fa fa-copyright" aria-hidden="true"></i> <a href="http://idav.ucdavis.edu/~okreylos/ResDev/SARndbox/index.html">Oliver Kreylos, UC Davis</a></small></p>
</section>
<!-- https://youtu.be/j9JXtTj0mzE -->

<!-- The Augmented REality Sandtable (ARES) -->
<section>
<h2>Augmented sandboxes</h2>
<img height="250px" src="img/history/ares_2.png">
<img height="250px" src="img/history/ares_3.png">
<p><b>The Augmented REality Sandtable (ARES), 2015-present</b></p>
<p><small>Charles R. Amburn, Nathan L. Vey, Michael W. Boyce, and MAJ Jerry R. Mize. 2015. The Augmented REality Sandtable ( ARES ). US Army Research Laboratory. ARL-SR-0340. DOI:<a href="http://dx.doi.org/10.13140/RG.2.1.2685.0006">http://dx.doi.org/10.13140/RG.2.1.2685.0006</a>
</small></p>
<p><small>Source: <i class="fa fa-copyright" aria-hidden="true"></i> US Army Research Laboratory</small></p>
</section>

<!-- TL -->

<!-- Tangible Landscape -->
<section data-background="img/system/tangible_landscape.jpg">
<h1 class="shadow">Tangible Landscape</h1>
<h3 class="shadow">A tangible user interface powered by open source GIS</h3>
<img height="150px" src="img/logos/logo_black.png">
<!--<p><small>Source: <i class="fa fa-creative-commons" aria-hidden="true"></i> <a href="https://tangible-landscape.github.io">NCSU GeoForAll Lab</a></small></p>-->
</section>

<!-- Real time interaction -->
<section>
<h2>Tanigble interaction with GIS</h2>
<video  data-autoplay class="stretch" style="margin-top: 1em" controls>
<source src="video/applications/tl_flow.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>With Tangible Landscape you can hold a GIS in your hands - feeling the shape of the earth, sculpting its topography, and directing the flow of water.</p>
</section>

<!-- How it works -->
<section>
<h2>How it works</h2>
<img class="stretch" src="img/system/system_schema.png">
<p>Tangible Landscape couples a digital and a physical model through a continuous cycle of 3D scanning, geospatial modeling, and projection</p>
</section>

<!-- Scanning -->
<section>
<h2>Realtime 3D scanning</h2>
<img class="stretch" src="img/interaction/fusion.jpg">
<p>with Kinect sensor</p>
</section>

<!-- Interactions -->
<section>
<h2>Interactions</h2>
<img class="stretch" src="img/interaction/interactions.png">
<table width="100%">
        <col width="18%">
        <col width="20%">
        <col width="20%">
        <col width="20%">
        <col width="20%">
        <tr>
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">surface
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">points
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">lines
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">areas
            <td style="vertical-align: middle; text-align:center; border-bottom: 0px; padding: 0;">areas
        </tr>
</table>
</section>

<!-- Features -->
<section>
<h2>Features</h2>
<video data-autoplay class="stretch" controls> <!--style="margin-top: 0.5em"-->
<source src="video/applications/tl_vr.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>A collaborative environment for tangible freeform modeling, object detection, real-time geospatial analytics, 3D rendering, and virtual reality</p>
<p><small>Payam Tabrizian, Anna Petrasova, Brendan Harmon, Vaclav Petras, Helena Mitasova, and Ross Meentemeyer. 2016. Immersive Tangible Geospatial Modeling. In Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems. GIS ’16. San Francisco, CA: ACM, 88:1-88:4. DOI:<a href="http://dx.doi.org/10.1145/2996913.2996950">http://dx.doi.org/10.1145/2996913.2996950</a>
</small></p>
</section>

<!-- VR
<section>
<h2>Tangible Landscape + Virtual Reality</h2>
<img class="stretch" src="img/system/system_schema_vr_blender.png">
<p>Realtime 3D rendering</p>
</section>

<section>
<h2>Tangible Landscape + Virtual Reality</h2>
<video  data-autoplay class="stretch" controls>
<source src="video/applications/tl_vr.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</section>
-->

<!-- Setup
<section>
    <h3>System setup</h3>
    <p>projector, scanner, stand, computer, model, table</p>
    <img width="30%" src="img/projector_configuration_2.png">
    <img width="30%" src="img/setup1.jpg">
    <p>Total budget: $2500
</section>
-->

<!-- Software
<section>
<h2>Software</h2>
<img class="stretch" src="img/system/software-schema.png">
</section>
-->

<!-- Interfaces
<section>
<h3>Interfaces</h3>
<img class="stretch" src="img/system/TUI_GUI_CLI_horizontal.jpg">
<p style="text-align:left">
<span style="margin-left:120px;margin-right:230px">TUI</span>
<span style="margin-right:300px">GUI</span>
<span style="margin-left:50px">API</span>
</section>
-->

<!-- Physical models -->

<!-- Visibility analysis -->
<section data-background-image="img/interaction/background_interaction_hands_markers.png">
<h2>Applications: visibility</h2>
<video  data-autoplay class="stretch" controls>
<source src="video/applications/tl_visibility.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>Visibility and line of sight</p>
</section>

<!-- Solar analysis-->
<section data-background-image="img/interaction/background_interaction_hands.png">
<h2>Applications: solar analysis</h2>
<img width="32%" src="img/applications/solar_1.jpg">
<img width="32%" src="img/applications/solar_2.jpg">
<img width="32%" src="img/applications/solar_3.jpg">
<!--<img width="32%" src="img/applications/solar.gif">-->
<p>Solar irradiation and cast shadows</p>
</section>

<!-- Trail Planning -->
<section data-background-image="img/interaction/background_interaction_hands_markers.png">
<h2>Applications: trail planning</h2>
<img width="32%" src="img/applications/trail_1.jpg">
<img width="32%" src="img/applications/trail_2.jpg">
<img width="32%" src="img/applications/trail_4.jpg">
<p>Optimized trail routing between waypoints based on energetics, topography, and cost maps with feedback including trail slopes and viewsheds</p>
</section>

<!-- Soil moisture -->
<section data-background-image="img/interaction/background_interaction_hands_markers.png">
<h2>Applications: 3D soil moisture exploration</h2>
<img height="190px" src="img/applications/subsurface_1.jpg">
<img height="190px" src="img/applications/subsurface_2.jpg">
<img height="190px" src="img/applications/subsurface_3.jpg">
<!--<img width="80%" src="img/applications/subsurface_cross_section.png">-->
</section>

<!-- Fire -->
<section data-background-image="img/interaction/background_interaction_hands.png">
<h2>Applications: wildfire spread</h2>
<video data-autoplay class="stretch" controls>
<source src="video/applications/tl_fire.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>Designing and testing fire breaks</p>
</section>

<!-- Erosion control
<section data-background-image="img/interaction/background_interaction_hands.png">
<h2>Applications: erosion control</h2>
<p>Sculpting a check dam to retain storm water and reduce erosion
<img width="32%" src="img/felt/felt_4.jpg">
<img width="32%" src="img/felt/checkdam_1.jpg">
<img width="32%" src="img/felt/checkdam_2.jpg">
</section>
-->

<!-- Erosion control -->
<section data-background-image="img/interaction/background_interaction_felt.png">
<h2>Applications: erosion control</h2>
<!--<img width="900px" src="img/felt/felt.png">-->
<img width="23%" src="img/felt/felt_1.jpg">
<img width="23%" src="img/felt/felt_2.jpg">
<img width="23%" src="img/felt/felt_3.jpg">
<img width="23%" src="img/felt/felt_4.jpg">
<p>Modifying land cover with colored felt
<!-- Adding grass (light green) and patches of trees (darker green)
changes the c-factor thus reducing erosion. -->
</p>
</section>

<!-- Sudden Oak Death
<section class="textimg">
<h2>Applications: Sudden Oak Death management</h2>
<div>
<p> Using Tangible Landscape in participatory modeling:
  <ul style="">
    <li>participants together explore options
    to slow down the spread of SOD in Sonoma
  using tangible user interface</li>
    <li>stochastic, spatially-explicit model (Meentemeyer et al. 2011) implemented in R (by F. Tonini)</li>
  </ul>
</div>
<img width=40% src="img/baseline_2014.gif">
</section>
<section data-background-image="img/interaction/background_interaction_markers.png">
<h2>Applications: Sudden Oak Death management</h2>
<iframe data-autoplay width="853" height="480" src="https://www.youtube.com/embed/dnOhOFHAkEU?rel=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>
</section>
-->

<!-- Futures -->
<section data-background-image="img/interaction/background_interaction_sand_laser.png">
<h2>Applications: urban growth</h2>
<video data-autoplay height="400px" controls>
<source src="video/applications/tl_futures.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>Simulation of urban growth scenarios with FUTURES model</p>
</section>

<!-- Coastal flooding -->
<section data-background-image="img/interaction/background_interaction_hands.png">
<h2>Applications: coastal flooding</h2>
<img width="32%" src="img/applications/tl_coastal_1.png">
<img width="32%" src="img/applications/tl_coastal_2.png">
<!--<img width="22%" src="img/tl_coastal_3s.png">-->
<img width="32%" src="img/applications/tl_coastal_4.png">
<p>Save houses from coastal flooding by building coastal defenses</p>
<p><small>Structured problem-solving with rules, challenging objectives, and scoring</small></p>
</section>

<!-- Termite invasion -->
<section data-background-image="img/interaction/background_interaction_markers.png">
<h2>Applications: Termite infestation</h2>
<img width="45%" src="img/applications/termite_game_2.jpg">
<img width="45%" src="img/applications/termite_game_3.jpg">
<p>Manage the spread of termites across a city by treating city blocks
using a model of biological invasion in R</p>
</section>

<!-- Serious gaming-->
<section data-background-image="img/applications/coffee_and_viz.jpg">
<h1 class="shadow">Serious gaming</h1>
<h3 class="shadow">with Tangible Landscape</h3>
</section>

<!-- Experiment -->
<section data-background-image="img/experiments/difference_1.jpg"> <!--carla_proj_aug.jpg-->
<h1 class="shadow">Experiment</h1>
<h3 class="shadow">Cognitively Grasping Topography with Tangible Landscape</h3>
</section>

<!-- Research questions -->
<section>
<h2>Research questions</h2>
<p><b>Many of the theoretical underpinnings of tangibles remain unproven and unexplored</b></p>
<ul>
<li>Do current approaches to tangible interfaces work as theorized?</li>
<li>Can users successfully cognitively grasp digital data as an extension of their bodies, intuitively interact, and offload cognitive processes with tangibles?</li>
<li>How does this change how users think and perform? How does it mediate spatial cognition and performance?</li>
<li>Can tangible interfaces for geospatial modeling enhance spatial thinking through kinaesthetic interaction with spatial computations?</li>
<li>Can users offload enough cognitive work onto their bodies to successfully parse and learn from computational feedback without suffering cognitive overload?</li>
</ul>
</section>

<!-- Research objectives -->
<section>
<h2>Research objectives</h2>
<ul>
<li>Design an effective tangible interface for geospatial modeling</li>
<li>Test whether coupling a physical and digital model of topography can improve 3D
spatial performance</li>
<li>Study how different geospatial analytics mediate users’ 3D spatial performance
when using a tangible interface for geospatial modeling</li>
</ul>
</section>

<!-- Experiment design -->
<section>
<h2>Experiment design</h2>
<ul>
<li>Digital 3D modeling</li>
<li>Analog 3D modeling</li>
<li>Augmented 3D modeling</li>
<li>Tangible 3D modeling</li>
</ul>
</section>

<!-- Digital -->
<section>
<h2>Digital 3D modeling</h2>
<video data-autoplay class="stretch" controls>
<source src="video/experiment/digital modeling.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>with Rhinoceros</p>
</section>

<!-- Analog -->
<section>
<h2>Analog 3D modeling</h2>
<video data-autoplay class="stretch" controls>
<source src="video/experiment/analog modeling.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>with polymeric sand</p>
</section>

<!-- Augmented -->
<section>
<h2>Projection augmented 3D modeling</h2>
<video data-autoplay class="stretch" controls>
<source src="video/experiment/augmented modeling.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>with Tangible Landscape</p>
</section>

<!-- Difference -->
<section>
<h2>3D modeling with the difference analytic</h2>
<video data-autoplay class="stretch" controls>
<source src="video/experiment/difference modeling.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>with Tangible Landscape</p>
</section>

<!-- Water flow -->
<section>
<h2>3D modeling with the water flow analytic</h2>
<video data-autoplay class="stretch" controls>
<source src="video/experiment/water flow modeling.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
<p>with Tangible Landscape</p>
</section>

<!-- Participants -->
<section>
<h2>Participants</h2>
<table style="margin-top: 0.5em">
<tr><th>Group</th><th>No.</th><th>GIS training</th><th>3D modeling expertise</th></tr>
<tr><td>Landscape architecture students</td><td>6</td><td>0</td><td>0</td></tr>
<tr><td>GIS students</td><td>6</td><td>6</td><td>0</td></tr>
<tr><td>Academics &amp; professionals</td><td>6</td><td>0</td><td>2</td></tr>
</table>
</section>

<!-- Results -->

<!-- All participants -->
<section>
<h2>All participants</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Reference</th><th>Digital</th><th>Analog</th><th>Augmented</th></tr>
<tr>
  <td word-wrap:break-word>Elevation <img src="img/legends/elevation_legend_1.png"></td>
  <td><img src="img/render_3d/participants/dem_1.png"></td>
  <td><img src="img/render_3d/participants/mean_dem_1.png"></td>
  <td><img src="img/render_3d/participants/mean_dem_2.png"></td>
  <td><img src="img/render_3d/participants/mean_dem_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Stdev. of differences<img src="img/legends/stdev_diff_legend.png"></td>
  <td><img src="img/render_3d/participants/dem_difference_1.png"></td>
  <td><img src="img/render_3d/participants/stdev_regression_difference_series_1.png"></td>
  <td><img src="img/render_3d/participants/stdev_regression_difference_series_2.png"></td>
  <td><img src="img/render_3d/participants/stdev_regression_difference_series_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Landforms<img src="img/legends/forms_legend.png"></td>
  <td><img src="img/render_3d/participants/forms_1.png"></td>
  <td><img src="img/render_3d/participants/mean_forms_1.png"></td>
  <td><img src="img/render_3d/participants/mean_forms_2.png"></td>
  <td><img src="img/render_3d/participants/mean_forms_3.png"></td>
</tr>
</table>
</section>

<!-- 3D novices -->
<section>
<h2>3D novices</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Reference</th><th>Digital</th><th>Analog</th><th>Augmented</th></tr>
<tr>
  <td word-wrap:break-word>Elevation <img src="img/legends/elevation_legend_1.png"></td>
  <td><img src="img/render_3d/3d_novices/dem_1.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_dem_1.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_dem_2.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_dem_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Stdev. of differences <img src="img/legends/stdev_diff_legend.png"></td>
  <td><img src="img/render_3d/3d_novices/dem_difference_1.png"></td>
  <td><img src="img/render_3d/3d_novices/stdev_regression_difference_series_1.png"></td>
  <td><img src="img/render_3d/3d_novices/stdev_regression_difference_series_2.png"></td>
  <td><img src="img/render_3d/3d_novices/stdev_regression_difference_series_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Landforms <img src="img/legends/forms_legend.png"></td>
  <td><img src="img/render_3d/3d_novices/forms_1.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_forms_1.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_forms_2.png"></td>
  <td><img src="img/render_3d/3d_novices/mean_forms_3.png"></td>
</tr>
</table>
</section>

<!-- 3D experts -->
<section>
<h2>3D experts</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Reference</th><th>Digital</th><th>Analog</th><th>Augmented</th></tr>
<tr>
  <td word-wrap:break-word>Elevation <img src="img/legends/elevation_legend_1.png"></td>
  <td><img src="img/render_3d/3d_experts/dem_1.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_dem_1.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_dem_2.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_dem_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Stdev. of differences<img src="img/legends/stdev_diff_legend.png"></td>
  <td><img src="img/render_3d/3d_experts/dem_difference_1.png"></td>
  <td><img src="img/render_3d/3d_experts/stdev_regression_difference_series_1.png"></td>
  <td><img src="img/render_3d/3d_experts/stdev_regression_difference_series_2.png"></td>
  <td><img src="img/render_3d/3d_experts/stdev_regression_difference_series_3.png"></td>
</tr>
<tr>
  <td word-wrap:break-word>Landforms<img src="img/legends/forms_legend.png"></td>
  <td><img src="img/render_3d/3d_experts/forms_1.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_forms_1.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_forms_2.png"></td>
  <td><img src="img/render_3d/3d_experts/mean_forms_3.png"></td>
</tr>
</table>
</section>

<!-- Difference -->
<section>
<h2>Difference</h2>
<img width="45%" src="img/experiments/difference_1.jpg">
<img width="45%" src="img/experiments/difference_2.jpg">
</section>

<!-- Difference results-->
<section>
<h2>Difference</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Elevation</th><th>Difference</th><th>Slope</th><th>Landforms</th></tr>
<tr>
  <td>Reference</td>
  <td><img src="img/render_3d/participants/dem_4.png"></td>
  <td><img src="img/render_3d/participants/dem_difference_4.png"></td>
  <td><img src="img/render_3d/participants/slope_4.png"></td>
  <td><img src="img/render_3d/participants/forms_4.png"></td>
</tr>
<tr>
  <td>Mean</td>
  <td><img src="img/render_3d/participants/mean_dem_4.png"></td>
  <td><img src="img/render_3d/participants/mean_dem_difference_4.png"></td>
  <td><img src="img/render_3d/participants/mean_slope_4.png"></td>
  <td><img src="img/render_3d/participants/mean_forms_4.png"></td>
</tr>
<tr>
  <td></td>
  <td><img src="img/legends/elevation_legend_4.png"></td>
  <td><img src="img/legends/diff_legend.png"></td>
  <td><img src="img/legends/slope_legend.png"></td>
  <td><img src="img/legends/forms_legend.png"></td>
</tr>
</table>
</section>

<!-- Difference: novices vs. experts-->
<section>
<h2>Difference</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Elevation</th><th>Difference</th><th>Slope</th><th>Landforms</th></tr>
<tr>
  <td>Reference</td>
  <td><img src="img/render_3d/participants/dem_4.png"></td>
  <td><img src="img/render_3d/participants/dem_difference_4.png"></td>
  <td><img src="img/render_3d/participants/slope_4.png"></td>
  <td><img src="img/render_3d/participants/forms_4.png"></td>
</tr>
<tr>
  <td>Mean</td>
  <td><img src="img/render_3d/participants/mean_dem_4.png"></td>
  <td><img src="img/render_3d/participants/mean_dem_difference_4.png"></td>
  <td><img src="img/render_3d/participants/mean_slope_4.png"></td>
  <td><img src="img/render_3d/participants/mean_forms_4.png"></td>
</tr>
<tr>
  <td></td>
  <td><img src="img/legends/elevation_legend_4.png"></td>
  <td><img src="img/legends/diff_legend.png"></td>
  <td><img src="img/legends/slope_legend.png"></td>
  <td><img src="img/legends/forms_legend.png"></td>
</tr>
</table>
</section>

<!-- Water flow -->
<section>
<h2>Water flow</h2>
<img class="stretch" src="img/experiments/tl_water.jpg">
</section>

<!-- Water flow results-->
<section>
<h2>Water flow</h2>
<table style="margin-top: 0.5em">
<tr><th></th><th>Elevation</th><th>Water depth</th><th>Depth difference</th></tr>
<tr>
  <td>Reference</td>
  <td><img width="70%" src="img/render_3d/participants/dem_5.png"></td>
  <td><img width="70%" src="img/render_3d/participants/depth_5.png"></td>
  <td><img width="70%" src="img/render_3d/participants/dem_difference_5.png"></td>
</tr>
<tr>
  <td>Mean</td>
  <td><img width="70%" src="img/render_3d/participants/mean_dem_5.png"></td>
  <td><img width="70%" src="img/render_3d/participants/mean_depth_5.png"></td>
  <td><img width="70%" src="img/render_3d/participants/mean_depth_difference_5.png"></td>
</tr>
<tr>
  <td></td>
  <td><img src="img/legends/elevation_legend_5.png"></td>
  <td><img src="img/legends/depth_legend.png"></td>
  <td><img src="img/legends/depth_diff_legend.png"></td>
</tr>
</table>
</section>



<!--  -->



<!-- Publications -->

<!-- Book -->
<section>
<h2>Publications</h2>
<img width="20%" src="img/publications/tl_book_cover.png">
<p><small>Anna Petrasova, Brendan Harmon, Vaclav Petras, and Helena Mitasova. 2015. Tangible Modeling with Open Source GIS, Springer International Publishing. DOI:<a href="http://dx.doi.org/10.1007/978-3-319-25775-4">http://dx.doi.org/10.1007/978-3-319-25775-4</a>
</small></p>
</section>

<!-- Papers -->
<section>
<h2>Publications</h2>
<p><small>Anna Petrasova, Brendan A. Harmon, Vaclav Petras, and Helena Mitasova. 2014. GIS-based environmental modeling with tangible interaction and dynamic visualization. In D. P. Ames & N. Quinn, eds. Proceedings of the 7th International Congress on Environmental Modelling and Software. San Diego, California, USA: International Environmental Modelling and Software Society.</small></p>
<p><small>Brendan A. Harmon. 2016. Embodied Spatial Thinking in Tangible Computing. In TEI ’16: Proceedings of the Tenth International Conference on Tangible, Embedded, and Embodied Interaction. Eindhoven, Netherlands: ACM Press, 693–696. DOI:<a href="http://dx.doi.org/10.1145/2839462.2854103">http://dx.doi.org/10.1145/2839462.2854103</a></small></p>
<p><small>Brendan A. Harmon, Anna Petrasova, Vaclav Petras, Helena Mitasova, and Ross K. Meentemeyer. 2016. Tangible Landscape: cognitively grasping the flow of water. In The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences. Prague: International Society of Photogrammetry and Remote Sensing. DOI:<a href="http://dx.doi.org/10.5194/isprsarchives-XLI-B2-647-2016">http://dx.doi.org/10.5194/isprsarchives-XLI-B2-647-2016</a></small></p>
<p><small>Payam Tabrizian, Anna Petrasova, Brendan Harmon, Vaclav Petras, Helena Mitasova, and Ross Meentemeyer. 2016. Immersive Tangible Geospatial Modeling. In Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems. GIS ’16. San Francisco, CA: ACM, 88:1--88:4. DOI:<a href="http://dx.doi.org/10.1145/2996913.2996950">http://dx.doi.org/10.1145/2996913.2996950</a></small></p>
<!--<p><small>Brendan A. Harmon et al. 2017. Cognitively Grasping Topography with Tangible Landscape. ACM Trans. Comput. Interact. (In review).<a href=""></a></small></p>-->
</section>

<!-- Future work -->

<!-- TL + robotic fabrication -->
<section>
<h2>Tangible Landscape with robotic fabrication</h2>
<img class="stretch" src="img/system/system_schema_robot.png">
<p>In-situ robotic fabrication for Tangible Landscape</p>
</section>

<!-- TL + robotic fabrication + streaming data + autonomous construction -->
<section>
<h2>Tangible Landscape with robotic fabrication, streaming data, and autonomous construction</h2>
<img  class="stretch" src="img/system/system_schema_land.png">
<p>Bi-directionally coupling physical and digital landscapes</p>
</section>

<!-- Appendix -->

<!-- Setup -->

<!-- Equipment -->

<!-- Speed and accuracy -->

<!--  -->

<!--  -->

<!--  -->
